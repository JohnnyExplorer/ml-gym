{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " democratized NLP\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "import torch\n",
    "import torch\n",
    "from transformers import LongformerTokenizerFast\n",
    "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\n",
    "    \"valhalla/longformer-base-4096-finetuned-squadv1\"\n",
    ")\n",
    "model = LongformerForQuestionAnswering.from_pretrained(\n",
    "    \"valhalla/longformer-base-4096-finetuned-squadv1\"\n",
    ")\n",
    "\n",
    "# # Comment out for inference on CPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # print (\"device \",device)\n",
    "# model = model.to(device)\n",
    "\n",
    "text = \"Huggingface has democratized NLP. Huge thanks to Huggingface for this.\"\n",
    "question = \"What has Huggingface done ?\"\n",
    "encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "start_scores = outputs[\"start_logits\"]\n",
    "end_scores = outputs[\"end_logits\"]\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "answer_tokens = all_tokens[torch.argmax(start_scores):torch.argmax(end_scores) + 1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "print(answer)\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 694/694 [00:00<00:00, 694kB/s]\n",
      "Downloading: 100%|██████████| 597M/597M [00:12<00:00, 48.6MB/s]\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 9.65MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 5.97MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 14.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n",
    "input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n",
    "\n",
    "# Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n",
    "global_attention_mask = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to global attention to be deactivated for all tokens\n",
    "global_attention_mask[:, [1, 4, 21,]] = 1  # Set global attention to random tokens for the sake of this example\n",
    "                                    # Usually, set global attention based on the task. For example,\n",
    "                                    # classification: the <s> token\n",
    "                                    # QA: question tokens\n",
    "                                    # LM: potentially on the beginning of sentences and paragraphs\n",
    "outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)\n",
    "sequence_output = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}